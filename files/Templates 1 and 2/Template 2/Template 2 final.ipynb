{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************************************************************\n",
      "Total input documents: 2\n",
      "************************************************************\n",
      "\n",
      "Textracting Document # 1: borelogs/Template2/WELL REPLACEMENT REPORT - T0609500432_27.pdf\n",
      "=============================================================================================\n",
      "Calling Textract...\n",
      "Started Asyc Job with Id: ae916fc8d5fa33bd8c6996e325380d862d80c0508f77a143795d985c1eacd7e9\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "Recieved Textract response...\n",
      "Generating output...\n",
      "Total Pages in Document: 1\n",
      "borelogs/Template2/WELL REPLACEMENT REPORT - T0609500432_27.pdf textracted successfully.\n",
      "\n",
      "Remaining documents: 1\n",
      "\n",
      "Textracting Document # 2: borelogs/Template2/WELL REPLACEMENT REPORT - T0609500432_30.pdf\n",
      "=============================================================================================\n",
      "Calling Textract...\n",
      "Started Asyc Job with Id: 56e5b93c3a63b481337d6aeef47bdc2132b01940fda8534d02ece27e000b6db8\n",
      "Job Status: IN_PROGRESS\n",
      "Job Status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "Recieved Textract response...\n",
      "Generating output...\n",
      "Total Pages in Document: 1\n",
      "borelogs/Template2/WELL REPLACEMENT REPORT - T0609500432_30.pdf textracted successfully.\n",
      "\n",
      "\n",
      "************************************************************\n",
      "Successfully textracted documents: 2\n",
      "************************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AWS Textract calls\n",
    "# for template 2\n",
    "\n",
    "\n",
    "! python C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\textractor\\\\textractor\\\\textractor.py --documents s3://ghd.datascience/borelogs/Template2/ --text --forms --tables --region us-west-1\n",
    "\n",
    "# Read all boring log files we have in S3 buckets\n",
    "\n",
    "#HAVE PDF COPY ON FOLDER AS WELL\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#print(glob.glob(\"*.pdf\"))\n",
    "#list of pdfs\n",
    "\n",
    "\n",
    "# Set os working directory\n",
    "                                                              \n",
    "pdflist = glob.glob(\"*.pdf\")\n",
    "\n",
    "#number of pdf's\n",
    "number_of_pdfs = len(pdflist)\n",
    "\n",
    "\n",
    "# creating a new directory for each pdf name\n",
    "for i in range(number_of_pdfs):\n",
    "    y = pdflist[i]\n",
    "    y = y[:-4]\n",
    "    \n",
    "    #glob.glob(y)\n",
    "    \n",
    "    os.mkdir(y)\n",
    "    z = y+'*'\n",
    "    arr = glob.glob(z)\n",
    "    #print(arr)\n",
    "    \n",
    "    len_array  = len(arr)\n",
    "    \n",
    "    \n",
    "    #moving same set of files to their respective directories\n",
    "    for j in range(len_array):\n",
    "            shutil.move(arr[j],y)\n",
    "\n",
    "    \n",
    "\n",
    "# Get all folders created\n",
    "folders = os.listdir()\n",
    "folders = [ x for x in folders if \".\" not in x ]\n",
    "\n",
    "# Number of folders\n",
    "\n",
    "num_of_folders = len(folders)\n",
    "\n",
    "folders\n",
    "\n",
    "#num_of_folders\n",
    "\n",
    "j = 0\n",
    "for i in range(num_of_folders):\n",
    "    #dir_name = \"C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\Topic Modelling\\\\boring_logs\\\\boring_logs\\\\contact depth\\\\test\\\\test\\\\\"+str(folders[i])\n",
    "    dir_name = str(folders[i])\n",
    "    test = os.listdir(dir_name)\n",
    "    #print(dir_name)\n",
    "    \n",
    "    \n",
    "    #remove all non csv files\n",
    "    for item in test:\n",
    "        if not item.endswith(\".csv\"):\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "            \n",
    "            \n",
    "    # Remove files that end with words.csv\n",
    "    for item in test:\n",
    "        if item.endswith(\"words.csv\"):\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "            \n",
    "            \n",
    "            \n",
    "    # Getting all file names that end with tables.csv\n",
    "    file_names_with_tables = []\n",
    "    for item in test:\n",
    "        if item.endswith(\"tables.csv\"):\n",
    "            file_names_with_tables.append(dir_name+\"\\\\\"+item)\n",
    "            \n",
    "            \n",
    "     \n",
    "    \n",
    "    \n",
    "    # remove first line of csv from tables.csv\n",
    "    #files = len(current_dir)\n",
    "    #i = 0\n",
    "    for item in file_names_with_tables:\n",
    "             with open(item,'r') as f:\n",
    "                new = \"new\"+str(j)+\".csv\"\n",
    "                with open(new,'w') as f1:\n",
    "                    next(f) # skip header line\n",
    "                    for line in f:\n",
    "                        f1.write(line)\n",
    "                             \n",
    "\n",
    "     # Creating meta data of files that end with forms.csv\n",
    "# rename files with forms.csv to  keyValues + str(i)for item in test:\n",
    "    for item in test:\n",
    "         if item.endswith(\"page-1-forms.csv\"):\n",
    "        #print(\"Yes\")\n",
    "            os.rename(os.path.join(dir_name, item), \"keyValues\"+str(j)+\".csv\")\n",
    "            \n",
    "    j = j+1       \n",
    "\n",
    "# empty folder\n",
    "    for item in test:\n",
    "        try:\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "        except:\n",
    "            continue;\n",
    "\n",
    "#move new's back to folder\n",
    "    work_directory = \"C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\Templates 1 and 2\\\\Template 2\\\\\"\n",
    "    wd = os.listdir(work_directory) \n",
    "    for item in wd:\n",
    "        if item.startswith(\"new\"):\n",
    "            shutil.move(item,dir_name)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Combining and creating combined to csv\n",
    "# this takes place only when there are files to be concatenated within each folder\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "\n",
    "    folders1 =  glob.glob('\"C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\Templates 1 and 2\\\\Template 2\\\\*')\n",
    "\n",
    "\n",
    "    for folder in folders1:\n",
    "        try:\n",
    "            os.chdir(folder)\n",
    "            extension = 'csv'\n",
    "            all_filenames = [i for i in glob.glob('new*.{}'.format(extension))]\n",
    "            #combine all files in the list\n",
    "            combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "            #export to csv\n",
    "            combined_csv.to_csv( \"combined.csv\", index=False)\n",
    "        except:\n",
    "            print(\"No files in folder to concateante\")\n",
    "            continue;\n",
    "\n",
    "    #move keyValue back to folder\n",
    "    #move news back to folder\n",
    "    os.chdir('C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\Templates 1 and 2\\\\Template 2\\\\')\n",
    "   \n",
    "    for item in wd:\n",
    "        if item.startswith(\"key\"):\n",
    "            shutil.move(item,dir_name)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "#move all csv's to folder\n",
    "\n",
    "root = \"C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\Templates 1 and 2\\\\Template 2\"\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if name.endswith('.csv'):\n",
    "            source=os.path.join(path, name)\n",
    "            destination = \"C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\Templates 1 and 2\\\\Template 2\"\n",
    "            dest = shutil.move(source, destination) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# AWS Textract calls\n",
    "# for template 1\n",
    "\n",
    "\n",
    "#! python C:\\\\Users\\\\spandalanghatbalasub\\\\Desktop\\\\textractor\\\\textractor.py --documents s3://ghd.datascience/borelogs/T0600101493.PDF --forms --tables --region us-west-1\n",
    "\n",
    "# Combine files, if in multiple pdf's\n",
    "# Logic\n",
    "# if names are same upto a point move to same folder\n",
    "\n",
    "# Arrange combined files in manner we want\n",
    "# new1 to new100.csv\n",
    "# md1 to md100.csv\n",
    "\n",
    "# Creating Empty Data frame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "column_names = [\"uscs\", \"Description\", \"Start Depth\",\"Finish Depth\",\"PROJECT NUMBER\"]\n",
    "\n",
    "df3 = pd.DataFrame(columns = column_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Dataframe structure of Metadata using 1 file\n",
    "\n",
    "\n",
    "md1 = pd.read_csv('keyValues1.csv')\n",
    "    \n",
    "    \n",
    "# Convert column to lower case and select key/ value\n",
    "md1.columns = map(str.lower, md1.columns)\n",
    "md1 = md1[['key','value']] \n",
    "md1.transpose()\n",
    "  \n",
    "    \n",
    "# Making the values above as column names\n",
    "md1 = md1.set_index('key').T\n",
    "\n",
    "\n",
    "md1.columns\n",
    "# trim values\n",
    "\n",
    "md1.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "md1 = md1[['DRILLING STARTED','DRILLING COMPLETED','PROJECT NUMBER']]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #Removing duplicate columns\n",
    "    # No need try except\n",
    "md1 = md1.loc[:,~md1.columns.duplicated()]\n",
    "\n",
    "# get length of csv's present\n",
    "# how many new's are there\n",
    "\n",
    "# Importing the os library\n",
    "import os\n",
    " \n",
    "# The path for listing items\n",
    "path = '.'\n",
    " \n",
    "# The list of items\n",
    "files = os.listdir(path)\n",
    "count_new = 0 \n",
    "# Loop to print each filename separately\n",
    "for filename in files:\n",
    "    if filename.startswith(\"new\"):\n",
    "        count_new = count_new + 1\n",
    "        \n",
    "        \n",
    "print(count_new)        \n",
    "\n",
    "\n",
    "# Add len(folder) in for loop\n",
    "\n",
    "for i in range(0,count_new):\n",
    "    \n",
    "    x= 'new'+str(i)+'.csv'\n",
    "    #print(x)\n",
    "    \n",
    "    \n",
    "    y= 'keyValues'+str(i)+'.csv'\n",
    "\n",
    "        #print(y)\n",
    "    # Metadata\n",
    "    # Reading key values from AWS Textract for Metadata configuration\n",
    "    # Always read first CSV from AWS to get md\n",
    "    #print(y)\n",
    "    md = pd.read_csv(y)\n",
    "    \n",
    "    \n",
    "    # Convert column to lower case and select key/ value\n",
    "    md.columns = map(str.lower, md.columns)\n",
    "    md = md[['key','value']] \n",
    "    md.transpose()\n",
    "    \n",
    "    \n",
    "    # Making the values above as column names\n",
    "    md = md.set_index('key').T\n",
    "\n",
    "\n",
    "    md.columns\n",
    "\n",
    "    # trim values\n",
    "\n",
    "    md.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    md = md[['DRILLING STARTED','DRILLING COMPLETED','PROJECT NUMBER']]\n",
    "    \n",
    "    \n",
    "    #md['PROJECT NUMBER']\n",
    "    \n",
    "    #Removing duplicate columns\n",
    "    # No need try except\n",
    "    md = md.loc[:,~md.columns.duplicated()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    md1 = md1.append(md)\n",
    "    \n",
    "    \n",
    "    ###################################################################\n",
    "    \n",
    "    ############# SECOND PART #####################################\n",
    "    \n",
    "    ##################################################################\n",
    "    \n",
    "    df = pd.read_csv(x)\n",
    "\n",
    "    df1= df\n",
    "    \n",
    "    \n",
    "    \n",
    "    df1 = df1.fillna(0)\n",
    "    df1 =df1.head(4)\n",
    "\n",
    "    # Drop columns that are 0\n",
    "    #Removing columns that are only 0, therby removing unwanted columns\n",
    "    df1= df1.loc[:, (df1 != 0).any(axis=0)]\n",
    "    \n",
    "    \n",
    "    #Iterating through all columns and checking which can be converted to float.\n",
    "    for col in df1.columns:\n",
    "        try:\n",
    "            df1[col] = df1[col].astype(float)\n",
    "        except:\n",
    "            continue;\n",
    "            \n",
    "            \n",
    "    # Selecting columns that are float\n",
    "    df1 = df1.select_dtypes(include=['float64'])\n",
    "\n",
    "    # Filling Na with 0\n",
    "    #df2 = df1.diff().fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Getting column where difference between subsequent rows for all values are positive.\n",
    "    # Since it is only the first few rows, the contact depth column will be positive and ruler will have negative values\n",
    "    df['Depth']= df[df1.columns]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Substituting with 0 since most columns except ones needed become 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    #Removing columns that are only 0, therby removing unwanted columns\n",
    "    df= df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "    #df_temp = df.head(3)\n",
    "    # Getting columns that contain SM. Add logic to include 'SM', 'SM ', 'ML' and so on\n",
    "    # Get list of common USCS\n",
    "\n",
    "    m = df.iloc[:, :-1].to_numpy() == 'SM '\n",
    "    n = df.iloc[:, :-1].to_numpy() == 'ML '\n",
    "    o = df.iloc[:, :-1].to_numpy() == 'ML'\n",
    "    p = m | n | o\n",
    "    df['matched_cols'] = [', '.join(df.columns[:-1][x]) for x in p]\n",
    "\n",
    "    x = df.matched_cols.unique().tolist()\n",
    "    x.remove('')\n",
    "    \n",
    "    '''df['new'] = df[x[0]]\n",
    "    df['new1'] = df[x[1]]\n",
    "    df['new'] = np.where(df['new'] == 0, df['new1'], df['new'])'''\n",
    "\n",
    "    df['USCS'] = df[x]\n",
    "    \n",
    "    #df['USCS'] = df['new']\n",
    "    \n",
    "    \n",
    "    # Removing white spaces in Depth column\n",
    "    #df['Depth'] = df['Depth'].str.strip()\n",
    "    \n",
    "    # If litholog, Description, geolog exists within column then rename column as Description\n",
    "    df.columns = df.columns.str.lower()\n",
    "    #df.columns = df.columns.str.replace('.*descrip', 'description')\n",
    "    df['Description'] = df[df.filter(regex='descrip|litholog|geolog').columns]\n",
    "    #df\n",
    "    \n",
    "    #ADD TRY EXCEPT HERE FOR TEMPLATES WHERE DEPTH IS IN STRING\n",
    "    \n",
    "    # 2 values in a column to next row\n",
    "\n",
    "    #s = df['depth'].str.split(' ').apply(pd.Series, 1).stack()\n",
    "    #s.index = s.index.droplevel(-1)\n",
    "    #s.name = 'depth'\n",
    "    #del df['depth']\n",
    "    #df =df.join(s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Selecting uscs, unnamed: 7 and Description in df1 and making df = 0 for memory\n",
    "    df1 = df[['uscs','depth', 'Description']]\n",
    "    df = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # If NaN previous value \n",
    "    df1['depth'] =df1['depth'].fillna(method='ffill')\n",
    "    df1['depth'] =   df1['depth'].astype(float)\n",
    "\n",
    "\n",
    "    # Create Start Depth Column\n",
    "    df1['Start'] = df1['depth'].shift()\n",
    "\n",
    "    # Renaming columns\n",
    "    df1.rename(columns={'depth': 'Finish Depth', 'Start':'Start Depth'}, inplace=True)\n",
    "\n",
    "    df1 = df1.fillna(0)\n",
    "    \n",
    "    \n",
    "    # Re- arrange columns\n",
    "    df1 = df1[['uscs','Description','Start Depth','Finish Depth']]\n",
    "    \n",
    "    df1['DS'] = md['DRILLING STARTED'][0]\n",
    "    df1['PROJECT NUMBER'] = md['PROJECT NUMBER'][0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df3 = df3.append(df1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "md1.to_csv('MD1.csv')\n",
    "\n",
    "df3.to_csv('df3.csv')\n",
    "\n",
    "md1 = 0\n",
    "df3 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
